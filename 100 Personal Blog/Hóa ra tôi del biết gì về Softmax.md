---
title: TÃ´i Ä‘Ã£ hiá»ƒu nháº§m em - Softmax
---

Vá»›i báº¥t cá»© ai há»c vá» Machine Learning, cháº¯c cháº¯n hÃ m sá»‘ sau Ä‘Ã£ in háº±n trong Ä‘áº§u báº¡n
$$
s(x_{i}) = \frac{e^{x_{i}}}{\sum_{j=1}^n e^{x_{j}}} 
$$
vÃ  hiá»ƒn nhiÃªn ta Ä‘á»u biáº¿t tÃªn cá»§a nÃ³ lÃ  **Softmax**.
HÃ m **Softmax** Ä‘Ã³ng vai trÃ² chÃ­nh trong viá»‡c chuyá»ƒn Ä‘á»•i káº¿t quáº£ Ä‘áº§u ra (logit) vá» dáº¡ng xÃ¡c suáº¥t (hÃ¬nh 1.1) vÃ  thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p (Multi-Class Classification)
![[Pasted image 20241006221149.png]]
<div style="text-align: center;"><em>HÃ¬nh 1.1: Má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘a lá»›p cÆ¡ báº£n</em></div>


NhÆ°ng thá»±c sá»± tÃ´i váº«n tháº¯c máº¯c vkl lÃ  táº¡i sao má»i ngÆ°á»i cá»© pháº£i dÃ¹ng cÃ¡i hÃ m cháº¿t tiá»‡t mang tÃªn **Softmax** nÃ y nhá»‰ ? CÃ¡c dáº¡ng hÃ m khÃ¡c Ä‘i mua sá»¯a bocon háº¿t rá»“i Ã  ? Thay vÃ¬ cháº¥p nháº­n vÃ  dÃ¹ng hÃ m **Softmax** nhÆ° tháº±ng wibu Ä‘á»™c thÃ¢n tuá»•i 20 chÆ°a tá»«ng náº¯m tay gÃ¡i vÃ  coi nÃ³ hiá»ƒn nhiÃªn lÃ  thá»© tá»‘i Æ°u nháº¥t hiá»‡n táº¡i bá»Ÿi bÃ i bÃ¡o xá»‹n xÃ² nÃ o cá»§a ngÃ nh AI hiá»‡n Ä‘áº¡i cÅ©ng dÃ¹ng cÃ¡i hÃ m bá» m* nÃ y:
- **AlexNet (2012):** Máº¡ng sÃ¢u + GPU = win
- **VGG16 (2014):** Nhiá»u layers thÃ¬ ngon hÆ¡n
- **Seq2seq Models (2014):** Máº¡ng neural cÃ³ thá»ƒ dá»‹ch Ä‘Æ°á»£c
- **ResNet (2015):** Há»c cÃ¡ch bá»• sung thÃ´ng tin cÃ²n thiáº¿u (residuals á»Ÿ Ä‘Ã¢y tÃ´i cÅ©ng khÃ´ng biáº¿t dá»‹ch nhÆ° nÃ o cho hay)
- **DenseNet (2017):** ResNet nhÆ°ng "chad"
- **Transformer (2017):** Attention is all you need to become a chad :Ä 
... vÃ  cÃ²n nhiá»u bÃ i khÃ¡c

thÃ¬ tÃ´i quyáº¿t Ä‘á»‹nh "lá»t há»‘" vÃ  tÃ¬m cÃ¡ch "váº¥y báº©n" cÃ¡i hÃ m cá»§a ná»£ nÃ y vÃ  trÃªn háº¿t lÃ  tráº£ lá»i Ä‘Æ°á»£c cÃ¢u há»i táº¡i sao bÃ i bÃ¡o quÃ¡i nÃ o ngÆ°á»i ta cÅ©ng dÃ¹ng con hÃ m nÃ y tháº¿, cÃ´ng nghiá»‡p vkl 
(*Spoiler: TÃ´i Ä‘Ã£ hiá»ƒu sai vá» nÃ³! *)

**ChÃº Ã½**: Äá»ƒ giáº£i thÃ­ch cho sá»± thÃ¹ Ä‘á»‹ch cá»§a tÃ´i vá»›i con hÃ m **Softmax** má»i báº¡n Ä‘á»c pháº§n I cá»§a bÃ i viáº¿t, hoáº·c náº¿u báº¡n khÃ´ng thÃ­ch cÃ³ thá»ƒ skip vÃ  Ä‘á»c luÃ´n pháº§n II náº¿u chá»‰ muá»‘n hiá»ƒu táº¡i sao cÃ¡c mÃ´ hÃ¬nh AI hiá»‡n nay Ä‘á»u sá»­ dá»¥ng con hÃ m nÃ y. 
## Sá»± thÃ¹ Ä‘á»‹ch cá»§a tÃ´i vá»›i con hÃ m $e^x$
#### Má»™t ná»­a lÃ½ do (hoÃ n toÃ n khÃ¡ch quan vÃ  khÃ´ng há» cÃ¡ nhÃ¢n)
TÃ´i vá»‘n lÃ  Ä‘á»©a khÃ¡ thÃ­ch ToÃ¡n (dÃ¹ trÃ¬nh ToÃ¡n nhÆ° loÃ¨n), thá»© tÃ´i thÃ­ch á»Ÿ nÃ³ lÃ  sá»± phá»©c táº¡p, khÃ³ hiá»ƒu nhÆ°ng áº©n Ä‘áº±ng sau lÃ  nÃ©t tinh táº¿, Ä‘áº¹p Ä‘áº½ cá»§a nhá»¯ng khuÃ´n máº«u tá»± nhiÃªn. Vá»›i tÃ´i, toÃ¡n há»c lÃ  ngÃ´n ngá»¯ cá»§a ChÃºa, lÃ  cÃ¢y cáº§u káº¿t ná»‘i ta vá»›i váº» Ä‘áº¹p ká»¹ vÄ© cá»§a thiÃªn nhiÃªn, táº¡o hÃ³a; pháº£n Ã¡nh sá»± thay Ä‘á»•i, káº¿t cáº¥u siÃªu hÃ¬nh cá»§a vÅ© trá»¥, ... nhÆ°ng trong váº» Ä‘áº¹p Ä‘áº§y trÃ¡ng lá»‡ áº¥y lÃ²i ra con hÃ m $e^x$.
Báº¡n khÃ´ng tin lÃ  nÃ³ dá»Ÿ ngÆ°á»i vÃ£i Ä‘Ã¡i Æ°, váº­y Ä‘á»ƒ tÃ´i chá»©ng minh cho báº¡n tháº¥y vá»›i bÃ i thÆ¡ sau

>*CÃ³ má»™t hÃ m sá»‘ trong toÃ¡n cung,*  
>*TÃªn lÃ  $e^x$, ná»•i vÃ´ cÃ¹ng.*  
>*Äá»©ng giá»¯a muÃ´n hÃ m, nÃ³ cao ngáº¡o,*  
>*VÃ¬ tá»± Ä‘áº¡o hÃ m, cháº³ng máº¥t cÃ´ng.*
>
>*HÃ m báº­c nháº¥t than: "Ta Ä‘Æ¡n sÆ¡,*  
>*NhÆ°ng ta dá»… hiá»ƒu, dá»… nÃªn thÆ¡."*  
>*$e^x$ cÆ°á»i kháº©y, flex báº¥t ngá»:*  
>*"Ta tÄƒng nhanh láº¯m, cháº³ng dá»«ng chá»."*
>
>*Äa thá»©c lÃªn tiáº¿ng: "Ta cÃ³ cÄƒn,*  
>*Giáº£i phÆ°Æ¡ng trÃ¬nh khÃ³, ta giÃºp pháº§n."*  
>*$e^x$ bÄ©u mÃ´i cÆ°á»i chÃª trÃ¡ch:*  
>*"CÄƒn lÃ m chi, ta Ä‘Ã¢u cÃ³ cáº§n!"*
>
>*Sin vá»›i Cosine nháº¹ nhÃ ng reo:*  
>*"ChÃºng ta lÃ  sÃ³ng, nhá»‹p mÃ£i theo."*  
>*$e^x$ kiÃªu cÄƒng: "CÃ¡c ngÆ°Æ¡i nhá»›,*  
>*Ta gáº¯n Euler, nÃªn ta trÃ¨o."*
>
>*Cuá»‘i cÃ¹ng, má»i hÃ m thá»Ÿ dÃ i than,*  
>*$e^x$ sao lÆ°á»i, cháº³ng biáº¿n toÃ n,*  
>*MÃ£i lÃ  chÃ­nh nÃ³, khÃ´ng Ä‘á»•i khÃ¡c,*  
>*Tá»± Ä‘áº¡o hÃ m xong, tá»± huy hoÃ ng.*

\- thi sá»¹ Du Mingyi

Báº£n cháº¥t cá»§a tá»± nhiÃªn luÃ´n thay Ä‘á»•i nhÆ°ng cÃ¡i hÃ m kiÃªu ngáº¡o cá»§a ná»£ nÃ y nÃ³ del chá»‹u thay Ä‘á»•i tÃ­ nÃ o cáº£, chÆ°a ká»ƒ táº¡i vÃ¬ cÃ¡i hÃ m cháº¿t tiá»‡t nÃ y mÃ  trong ká»³ thi THPTQG tÃ´i sai ngu m* máº¥t 1 cÃ¢u vÃ¬ báº¥m nháº§m $e^x$ thÃ nh $2^x$ (CÅ©ng khÃ´ng trÃ¡ch tÃ´i Ä‘Æ°á»£c vÃ¬ tÃ´i lÃ  dÃ¢n chÃ¢u Ã báº¥m A rá»“i cÃ¹ng láº¯m lÃ  B chá»© tá»± nhiÃªn báº£o báº¥m vÃ o $e$ thÆ°á»ng thÃ¬ tháº­t lÃ  cÃ³ lá»—i vá»›i tá»• tiÃªn).

CÃ²n gÃ¬ tá»‡ hÃ m má»™t hÃ m $e^x$ ngoÃ i má»™t hÃ m cáº¥u táº¡o bá»Ÿi nhiá»u hÃ m dáº¡ng $e^x$ - VÃ¢ng Ä‘Ã³ chÃ­nh lÃ  lÃ½ do tÃ´i ghÃ©t **Softmax** vaidai.
#### Ná»­a cÃ²n láº¡i ...
$e^x$ thá»±c sá»± lÃ  má»™t hÃ m khÃ´ng máº¥y "thÃ¢n thiá»‡n" vá»›i mÃ¡y tÃ­nh vÃ  cá»±c ká»³ dá»… trÃ n sá»‘. Tháº­t váº­y, báº¡n hÃ£y thá»­ Ä‘oáº¡n code sau
```python
torch.exp(torch.Tensor([100.0]))
```
Báº¡n Ä‘oÃ¡n xem káº¿t quáº£ tráº£ vá» lÃ  gÃ¬
```python
tensor([inf]) ğŸ’€ğŸ’€ğŸ’€
```
Viá»‡c tÃ­nh $e^{x}$ trÃªn mÃ¡y tÃ­nh lÃ  cá»±c ká»³ tá»‘n kÃ©m vÃ¬ mÃ¡y tÃ­nh chá»‰ cÃ³ thá»ƒ Æ°á»›c lÆ°á»£ng nÃ³ má»™t cÃ¡ch xáº¥p xá»‰ thÃ´ng thÆ°á»ng nháº¥t lÃ  thÃ´ng qua khai triá»ƒn Taylor (1), xáº¥p xá»‰ Pade hoáº·c CORDIC. 
$$
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots \hspace{5cm} (1)
$$
VÃ  Ä‘áº¿n Ä‘Ã¢y cháº¯c báº¡n cÅ©ng nháº­n ra sá»‘ láº§n pháº£i tÃ­nh $e^x$ cá»§a cÃ¡i hÃ m **Softmax** rá»“i nhá»‰ ğŸ˜µ .

NgoÃ i ra, rÃµ rÃ ng náº¿u vai trÃ² cá»§a lá»›p **Softmax** lÃ  chuyá»ƒn *logit* thÃ nh *xÃ¡c suáº¥t* thÃ¬ ta hoÃ n toÃ n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c dáº¡ng hÃ m khÃ¡c miá»…n Ä‘áº¥y lÃ  hÃ m dÆ°Æ¡ng, Ä‘Æ¡n Ä‘iá»‡u vÃ  kháº£ vi thÃ¬ Ä‘á»u Ä‘Æ°á»£c. Váº­y vÃ¬ sao **Softmax** láº¡i chá»n sá»­ dá»¥ng $e^x$ sau má»™t Ä‘á»‘ng lÃ½ do báº¥t lá»£i tÃ´i nÃ³i á»Ÿ trÃªn cÅ©ng nhÆ° vÃ¬ sao viá»‡c sá»­ dá»¥ng $\sum$ láº¡i hiá»‡u quáº£ vÃ  Ä‘Ãºng vá» máº·t toÃ¡n há»c. 

Sau máº¥y hÃ´m ngá»“i tÃ¬m tÃ²i, xem YouTube, Ä‘á»c blog chá»— nÃ y chá»— kia thay vÃ¬ lÃ m bÃ i trÃªn lá»›p thÃ¬ cuá»‘i cÃ¹ng tÃ´i cÅ©ng tÃ¬m ra lÃ½ do Ä‘á»§ thá»a Ä‘Ã¡ng Ä‘á»ƒ tráº£ lá»i tháº¯c máº¯c vá»› váº©n trÃªn, khÃ´ng dÃ i dÃ²ng ná»¯a ta cÃ¹ng báº¯t Ä‘áº§u vÃ o pháº§n chÃ­nh.

## VÃ¬ sao hÃ m Softmax láº¡i Ä‘Æ°á»£c sá»­ dá»¥ng ?
#### VÃ¬ sao dÃ¹ng $\frac{u(a)}{\sum_{a'\in A}u(a')}$
Do tÃ´i váº«n khÃ¡ ká»³ thá»‹ con hÃ m $e^x$ nÃªn tÃ´i sáº½ báº¯t Ä‘áº§u vá»›i lÃ½ do vÃ¬ sao viá»‡c **Softmax** mÃ´ hÃ¬nh hÃ³a dÆ°á»›i dáº¡ng 
$$
\frac{u(a)}{\sum_{a'\in A}u(a')}
$$
láº¡i Ä‘Ãºng Ä‘áº¯n vá» máº·t toÃ¡n há»c vÃ  mÃ´ táº£ Ä‘Æ°á»£c quan há»‡ xÃ¡c suáº¥t cáº§n thiáº¿t.

TrÆ°á»›c háº¿t, ta cáº§n thá»«a nháº­n trong cÃ¡c lá»›p bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p mÃ  ta hay lÃ m viá»‡c, tá»· lá»‡ xÃ¡c suáº¥t giá»¯a 2 lá»›p $i$ vÃ  $j$ sáº½ khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi sá»± xuáº¥t hiá»‡n hay biáº¿n máº¥t cá»§a cÃ¡c lá»›p khÃ¡c; vÃ­ dá»¥ nhÆ° lá»›p $k$ nÃ o Ä‘áº¥y. CÃ³ váº» váº«n hÆ¡i khÃ³ hiá»ƒu, ta Ä‘i vÃ o chi tiáº¿t hÆ¡n vá»›i má»™t bÃ i toÃ¡n cá»¥ thá»ƒ nhÆ° sau: Giáº£ sá»­ ta cÃ³ bÃ i toÃ¡n phÃ¢n loáº¡i áº£nh vÃ o 5 class 
$ChÃ³$, $MÃ¨o$, $GÃ $, $Lá»£n$, $BÃ²$, ta giáº£ Ä‘á»‹nh viá»‡c xÃ¡c Ä‘á»‹nh má»™t áº£nh lÃ  $ChÃ³$ hay $MÃ¨o$ khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi sá»± cÃ³ máº·t cá»§a cÃ¡c class cÃ²n láº¡i (rÃµ rÃ ng viá»‡c giáº£ Ä‘á»‹nh nhÆ° nÃ y lÃ  há»£p lÃ½ vÃ¬ viá»‡c quyáº¿t Ä‘á»‹nh 1 áº£nh lÃ  ChÃ³ hay MÃ¨o hoÃ n toÃ n khÃ´ng liÃªn quan tá»›i sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c class khÃ¡c) nÃªn khi viáº¿t dÆ°á»›i dáº¡ng toÃ¡n há»c ta cáº§n má»™t mÃ´ hÃ¬nh mÃ´ táº£
$$
\frac{P(ChÃ³)}{P(MÃ¨o)} = const
$$
báº¥t ká»ƒ cÃ³ sá»± hiá»‡n diá»‡n cá»§a cÃ¡c class khÃ¡c hay khÃ´ng. Viá»‡c cháº¥p thuáº­n giáº£ Ä‘á»‹nh trÃªn cÅ©ng chÃ­nh lÃ  ta Ä‘ang thá»«a nháº­n **Luce's choice axiom** hay nguyÃªn lÃ½ **"Independence from Irrelevant Alternatives" (IIA)** - cho ráº±ng sá»± lá»±a chá»n giá»¯a hai Ä‘á»‘i tÆ°á»£ng khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi sá»± cÃ³ máº·t cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng khÃ¡c trong táº­p lá»±a chá»n (Ä‘Ã³ cÅ©ng chÃ­nh lÃ  lÃ½ do vÃ¬ sao $\frac{P(ChÃ³)}{P(MÃ¨o)}= const$ báº¥t ká»ƒ xuáº¥t hiá»‡n GÃ , Lá»£n, BÃ² hay khÃ´ng)

---

>**Luce's choice axiom:** Consider a set $X$ of possible outcomes, and consider a selection rule $P$, such that for any $a \in A \subset X$ with $A$ a finite set, the selector $a$ from $A$ with probability $P(a|A)$
>
>*Axiom 1*: If $P(a|A) = 0, \hspace{0.2cm} P(b|A) >0,$ then for any $a, b\in B \subset A,$ we still have $P(a|B) = 0$
> 
>*Axiom 2*: $P(a|A) = P(a|B)\sum_{b\in B}P(b|A)$ for any $a \in B \subset A$

---

<div style="text-align: center;"><em>"TÃ´i sáº½ khÃ´ng Ä‘i vÃ o chi tiáº¿t hÆ¡n vá» cÃ¡c pháº§n chá»©ng minh toÃ¡n, mÃ  sáº½ chá»‰ Ä‘á»ƒ Ä‘á»‹nh nghÄ©a tiÃªn Ä‘á» á»Ÿ Ä‘Ã¢y Ä‘á»ƒ cÃ¡c báº¡n tham kháº£o, trong bÃ i nÃ y báº¡n chá»‰ cáº§n náº¯m Ä‘Æ°á»£c ráº±ng tiÃªn Ä‘á» trÃªn thá»ƒ hiá»‡n sá»± lá»±a chá»n giá»¯a hai Ä‘á»‘i tÆ°á»£ng khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi sá»± cÃ³ máº·t cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng khÃ¡c trong táº­p lá»±a chá»n"</em></div>

Vá» máº·t toÃ¡n há»c ngÆ°á»i ta cÅ©ng chá»©ng minh Ä‘Æ°á»£c ráº±ng vá»›i *matching law* $P(a|A) = \frac{u(a)} {\sum_{a'\in A}u(a')}$ thá»a mÃ£n **Luce's choice axiom** nhÆ° váº­y cÃ³ thá»ƒ tháº¥y hÃ m **Softmax** thá»a mÃ£n tiÃªn Ä‘á» trÃªn. 

Quay láº¡i bÃ i toÃ¡n phÃ¢n loáº¡i áº£nh Ä‘Ã£ nÃ³i á»Ÿ trÃªn, giáº£ sá»­ á»Ÿ lá»›p *logits* ta khÃ´ng xÃ©t Ä‘áº¿n sá»± hiá»‡n diá»‡n cá»§a GÃ , Lá»£n BÃ² vÃ  ta cÃ³:
- ChÃ³: 8 
- MÃ¨o: 6 

MÃ´ hÃ¬nh hÃ³a dÆ°á»›i dáº¡ng xÃ¡c suáº¥t báº±ng $\frac{u(a)}{\sum_{a'\in A}u(a')}$ ta thu Ä‘Æ°á»£c:
- $P(ChÃ³) = \frac{8}{6+8} = \frac{4}{7}$
- $P(MÃ¨o) = \frac{6}{8+6} = \frac{3}{7}$

tá»« Ä‘Ã¢y ta thu Ä‘Æ°á»£c tá»‰ sá»‘ 
$$
\frac{P(ChÃ³)}{P(MÃ¨o)} = \frac{4}{3}
$$
Ta bá»• sung thÃªm cÃ¡c giÃ¡ trá»‹ cá»§a GÃ , Lá»£n, BÃ² vÃ o mÃ´ hÃ¬nh:
- ChÃ³: 8
- MÃ¨o: 6
- GÃ : 4
- Lá»£n: 5
- BÃ²: 9

LÃºc nÃ y tiáº¿p tá»¥c mÃ´ hÃ¬nh hÃ³a dÆ°á»›i dáº¡ng xÃ¡c suáº¥t ta thu Ä‘Æ°á»£c:
- $P(ChÃ³) = \frac{8}{8+6+4+5+9} = \frac{1}{4}$
- $P(MÃ¨o) = \frac{6}{8+6+4+5+9} = \frac{3}{16}$

tiáº¿p tá»¥c tÃ­nh thá»­ cáº·p tá»‰ sá»‘ $\frac{P(ChÃ³)}{P(MÃ¨o)}$ ta nháº­n ra giÃ¡ trá»‹ cáº·p tá»‰ sá»‘ nÃ y khÃ´ng Ä‘á»•i
$$
\frac{P(ChÃ³)}{P(MÃ¨o)}=\frac{4}{3}
$$

NhÆ° váº­y dáº¡ng $\frac{u(a)}{\sum_{a'\in A}u(a')}$ giÃºp Ä‘áº£m báº£o viá»‡c lá»±a chá»n (tá»‰ sá»‘ xÃ¡c suáº¥t) giá»¯a hai Ä‘á»‘i tÆ°á»£ng trong khÃ´ng gian máº«u lÃ  khÃ´ng Ä‘á»•i báº¥t ká»ƒ cÃ³ sá»± cÃ³ máº·t cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng khÃ¡c trong táº­p lá»±a chá»n hay khÃ´ng.

Äáº¿n Ä‘Ã¢y, ta láº¡i Ä‘áº·t ra má»™t cÃ¢u há»i khÃ¡c: Váº­y náº¿u ta muá»‘n mÃ´ hÃ¬nh hÃ³a lá»›p bÃ i toÃ¡n mÃ  á»Ÿ Ä‘Ã³ viá»‡c lá»±a chá»n giá»¯a hai Ä‘á»‘i tÆ°á»£ng báº¥t ká»³ thay Ä‘á»•i vá»›i sá»± cÃ³ máº·t cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng khÃ¡c thÃ¬ lÃ m nhÆ° nÃ o ? VÃ­ dá»¥ ta cÃ³ bÃ i toÃ¡n **Image tagging** vá»›i 3 Ä‘á»‘i tÆ°á»£ng: $Beach, Sunset, Ocean$ rÃµ rÃ ng sá»± cÃ³ máº·t cá»§a $Ocean$ tÄƒng *likelihood* (kháº£ nÄƒng xáº£y ra) cá»§a $Beach$ vÃ  tá»« Ä‘Ã³ lÃ m thay Ä‘á»•i tá»‰ sá»‘ giá»¯a $\frac{P(Beach)}{P(Sunset)}$. Vá»›i lá»›p bÃ i toÃ¡n nÃ y ngÆ°á»i ta sá»­ dá»¥ng dáº¡ng hÃ m **Hierarchical Softmax** mÃ  tÃ´i sáº½ nÃ³i tá»›i trong nhá»¯ng bÃ i tiáº¿p theo khi dÃ¹ng hÃ m nÃ y Ä‘á»ƒ tá»‘i Æ°u cho mÃ´ hÃ¬nh **Word2Vec**.

#### VÃ¬ sao láº¡i dÃ¹ng $e^x$
Pheww... cuá»‘i cÃ¹ng cÅ©ng tá»›i lÃ½ do pháº£i dÃ¹ng tá»›i hÃ m cá»§a ná»£ nÃ y. TrÆ°á»›c háº¿t trong cÃ¡c bÃ i toÃ¡n Há»c mÃ¡y nÃ³i chung má»¥c tiÃªu cá»§a ta lÃ  pháº£i tÃ´i Æ°u má»™t hÃ m loss kiá»ƒu $L(y, f(s))$. XÃ©t má»™t bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p, $y$ lÃ  one-hot vector cá»§a cÃ¡c class cÃ²n $f(s)$ lÃ  Ä‘áº§u ra cá»§a hÃ m **Softmax** ($\sigma(s))$ vá»›i $s$ lÃ  *logits*.
Äá»ƒ tá»‘i Æ°u hÃ m trÃªn thÃ´ng thÆ°á»ng ta sáº½ pháº£i tÃ­nh gradient 
$$
\frac{ \partial L(y, f(s)) }{ \partial s } = \left[ \frac{ \partial f(s) }{ \partial s }\right]^T \frac{ \partial L(y, f(s)) }{ \partial f(s) }  \hspace{1cm} (\text{chain rule}) (2)
$$
CÃ³ thá»ƒ tháº¥y trong cÃ¡c bÃ i toÃ¡n phÃ¢n lá»›p, ngÆ°á»i ta cÅ©ng thÆ°á»ng chá»n hÃ m máº¥t mÃ¡t $L(y, f(s)) = -y^T\log(f(s))$ (Negative Log-likelihood); viá»‡c chá»n nÃ y cÃ³ Ã½ nghÄ©a vÃ´ cÃ¹ng quan trá»ng vÃ¬ vá»›i viá»‡c giá»›i thiá»‡u hÃ m $\log$ vÃ o Ä‘Ã¢y ta Ä‘Ã£ biáº¿n Ä‘á»•i Ä‘Æ°á»£c hÃ m **Softmax** tá»« dáº¡ng Ä‘a thá»©c vá» dáº¡ng trÃ´ng "*tuyáº¿n tÃ­nh*" hÆ¡n
$$
\begin{align*}
L(y, f(s)) &= -y^T\log \frac{e^s}{\sum_{c}e^{s_{c}}} \\
&=-y^T\left( s - \log \sum_{c}e^{s_{c}} \right)
\end{align*}
$$
NgoÃ i ra khi tÃ­nh gradient thÃ¬ ta cÃ³:
$$
\begin{align*}
\frac{ \partial f(s) }{ \partial s } &=  \frac{ \partial \sigma(s) }{ \partial s }  \\
&=diag(\sigma(s)) - \sigma(s)\sigma(s)^T \hspace{1cm} (3)
\end{align*}
$$
(3) tráº£ vá» má»™t ma tráº­n Ä‘á»‘i xá»©ng vá»›i tá»•ng cá»§a tá»«ng hÃ ng báº±ng 0 (vÃ  Ä‘Æ°Æ¡ng nhiÃªn vÃ¬ Ä‘á»‘i xá»©ng nÃªn tá»•ng cÃ¡c cá»™t cÅ©ng báº±ng 0); Ä‘Ã¢y lÃ  má»™t tÃ­nh cháº¥t ráº¥t hay vÃ¬ nÃ³ cho phÃ©p ta thá»±c hiá»‡n trá»« Ä‘i gradient (VD dÃ¹ng thuáº­t toÃ¡n Gradient Descent) Ä‘á»ƒ cáº­p nháº­t tham sá»‘ mÃ  váº«n báº£o toÃ n tÃ­nh cháº¥t tá»•ng Ä‘áº§u ra báº±ng 1 cá»§a má»™t xÃ¡c suáº¥t.  (Báº¡n cá»© tÃ­nh thá»­ ra lÃ  biáº¿t)

KhÃ´ng chá»‰ dá»«ng láº¡i á»Ÿ Ä‘Ã³, viá»‡c tÃ­nh $\frac{ \partial L(y, f(s)) }{ \partial f(s) }$ vá»›i viá»‡c chá»n negative-log likelihood cÃ²n trá»Ÿ nÃªn cá»±c ká»³ dá»… dÃ ng
$$
\begin{align*}
\frac{ \partial L(y, f(s)) }{ \partial f(s) } &= -\frac{y}{\sigma(s)} \hspace{1cm}(4)
\end{align*}
$$
Tá»« (3) vÃ  (4), ta thu Ä‘Æ°á»£c 
$$
\begin{align*}
\frac{ \partial L(y, f(s)) }{ \partial s } &= \left[ \frac{ \partial f(s) }{ \partial s }\right]^T \frac{ \partial L(y, f(s)) }{ \partial f(s) } \\
&= (diag(\sigma(s)) - \sigma(s)\sigma(s)^T) \frac{-y}{\sigma(s)} \\
&= \sigma(s) - y \hspace{2.3cm} (5)
\end{align*}
$$
<b style="color: red">THáº¬T Sá»° QUÃ áº¢O MA</b>, ta Ä‘Ã£ chuyá»ƒn tá»« viá»‡c pháº£i tÃ­nh phÃ©p nhÃ¢n cá»§a 2 ma tráº­n vá»›i nhau trong biá»ƒu thá»©c (3) thÃ nh viá»‡c tÃ­nh phÃ©p trá»« cá»§a duy nháº¥t 2 vector báº±ng viá»‡c chá»n hÃ m loss vÃ  sá»­ dá»¥ng **Softmax** vÃ  Ä‘Ã¢y chÃ­nh lÃ  lÃ½ do vÃ¬ sao **Softmax** láº¡i Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u Ä‘áº¿n váº­y. TrÃªn thá»±c táº¿ khi chÃºng ta gá»i hÃ m 
```python
nn.CrossEntropyLoss
```
trong thÆ° viá»‡n Pytorch, thÃ¬ biáº¿u thá»©c trÃªn chÃ­nh xÃ¡c lÃ  cÃ¡ch mÃ  hÃ m nÃ y tÃ­nh toÃ¡n. Thá»±c sá»± tÃ´i Ä‘Ã£ hiá»ƒu láº§m em - **Softmax** tÃ´i xin lá»—i, tÃ´i sai rá»“i.

**TÃ¡i bÃºt:** NhÆ°ng tÃ´i váº«n ghÃ©t con hÃ m $e^x$ vkl nhÃ© !

## References
[Why Do Neural Networks Love the Softmax? (youtube.com)](https://www.youtube.com/watch?v=p-6wUOXaVqs)
[Softmax with cross-entropy (mattpetersen.github.io)](https://mattpetersen.github.io/softmax-with-cross-entropy)
[Understanding softmax and the negative log-likelihood (ljvmiranda921.github.io)](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)
[Luce's choice axiom - Wikipedia](https://en.wikipedia.org/wiki/Luce%27s_choice_axiom)