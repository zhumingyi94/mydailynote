**202409272212**
**Status:** #ReinforcementLearning  
**Tags:** 
<hr style="border: none; height: 2px; background-color: #000000; margin: 20px 0;">

# Multi-armed Bandits
<hr style="border: none; height: 2px; background-color: #000000; margin: 20px 0;">

### Key note
2.1 - 2.3 
- instructive feedback 
- evaluative feedback
- *nonassociative*
- stationary probability distribution $\pi = \pi P$ ($P$ is the transition matrix)
- Describe *k-armed* bandit problem
- $q_{*}(a) = ?$
- $Q_{t}(a) = ?$
- Action-value methods
- *sample-average* method
- Why $q_{*}(a)$ converge to $Q_{t}(a)$
- *greedy* action selection method:$A_{t} = ?$
- what is $\epsilon$-greedy method

2.4 
$NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]$
- What is *error*
-

**LAW OF LARGE NUMBER**

2,5 
- How can you tracking a nonstationary problem
- How to choose $\{\alpha_{n(a)}\}$ to ensure converge (2.7)
- 
$q_{*}(a) = \mathbb{E}[R_{t}|A_{t}=a]$



### Exercise


### References

